{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import plot_tools\n",
    "import utils\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, grab the data (randomly) from the newsgroup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: shmuel@mapsut.einstein.com (Shmuel Einstein)\n",
      "Subject: Screen capture -> CYMK converter\n",
      "Nntp-Posting-Host: mapsut.einstein.com\n",
      "Organization: Shmuel Einstein & Associates, Inc.\n",
      "Lines: 20\n",
      "\n",
      "I have a small program to extract a 640x480 image from a vga 16 color screen,\n",
      "and store that image in a TIFF file.  I need to insert the image into a\n",
      "sales brochure, which I then need printed in 4 color.  On a mac, I would\n",
      "use Photoshop to separate the image into 5 EPS files, and then pull it into\n",
      "quark express, then get it printed to film on a lintronix at a service bureau.\n",
      "\n",
      "However, I don't have a mac, but I do have windows.  What would I need to \n",
      "do this type of operation in the windows 3.1 environment?  Are there any\n",
      "separation programs available on the net?  Is there a good page layout program\n",
      "that I should look into?\n",
      "\n",
      "Thanks in advance.\n",
      "\n",
      "\n",
      "-- \n",
      "Shmuel Einstein, shmuel@einstein.com\n",
      "Shmuel Einstein & Associates, Inc.\n",
      "9100 Wilshire Blvd, Suite 235 E\n",
      "Beverly Hills, CA  90212\n",
      "310/273-8971 FAX 310/273-8872\n",
      "#$%&()*+-/;<=>@[\\]^_`{|}~\n",
      "#########################\n",
      "From: shmuel@mapsut\n",
      "einstein\n",
      "com (Shmuel Einstein)Subject: Screen capture -> CYMK converterNntp-Posting-Host: mapsut\n",
      "einstein\n",
      "comOrganization: Shmuel Einstein & Associates, Inc\n",
      "Lines: 20I have a small program to extract a 640x480 image from a vga 16 color screen,and store that image in a TIFF file\n",
      "I need to insert the image into asales brochure, which I then need printed in 4 color\n",
      "On a mac, I woulduse Photoshop to separate the image into 5 EPS files, and then pull it intoquark express, then get it printed to film on a lintronix at a service bureau\n",
      "However, I don't have a mac, but I do have windows\n",
      "What would I need to do this type of operation in the windows 3\n",
      "1 environment\n",
      "Are there anyseparation programs available on the net\n",
      "Is there a good page layout programthat I should look into\n",
      "Thanks in advance\n",
      "-- Shmuel Einstein, shmuel@einstein\n",
      "comShmuel Einstein & Associates, Inc\n",
      "9100 Wilshire Blvd, Suite 235 EBeverly Hills, CA  90212310/273-8971 FAX 310/273-8872\n"
     ]
    }
   ],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc',\n",
    "                       'comp.graphics', 'sci.space']\n",
    "\n",
    "x = fetch_20newsgroups(categories=categories)\n",
    "#print(x)\n",
    "random.seed(17)\n",
    "y = x.data[random.randrange(len(x.data))]\n",
    "#print(y.rstrip())\n",
    "sentances = utils.read_data(y)\n",
    "#print('#########################')\n",
    "for line in sentances[:20]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train NMF and LDA models on the document to get topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "tfidf_vectorizer.tokenizer = utils.tokenizer\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentances)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "tf_vectorizer.tokenizer = utils.tokenizer\n",
    "tf = tf_vectorizer.fit_transform(sentances)\n",
    "\n",
    "# Train the NMF topic model\n",
    "nmf = NMF(n_components=5, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Train the LDA topic model\n",
    "lda = LatentDirichletAllocation(n_topics=5, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the summarization with NMF topic model, trained on only the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set then number of sentances to return (for all models)\n",
    "num_out = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On a mac, I woulduse Photoshop to separate the image into 5 EPS files, and then pull it intoquark express, then get it printed to film on a lintronix at a service bureau\n",
      "Lines: 20I have a small program to extract a 640x480 image from a vga 16 color screen,and store that image in a TIFF file\n",
      "comOrganization: Shmuel Einstein & Associates, Inc\n",
      "comShmuel Einstein & Associates, Inc\n",
      "com (Shmuel Einstein)Subject: Screen capture -> CYMK converterNntp-Posting-Host: mapsut\n"
     ]
    }
   ],
   "source": [
    "#Rank sentance importance by number of nouns\n",
    "summarizer_nmf = utils.TextSummary(nmf, num_out, vectorizer=tfidf_vectorizer, method='nns')\n",
    "for line in summarizer_nmf.extract_summary(sentances):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-45b751b7e0a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummarizer_nmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_nns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/eoshea/sflintro/text_sum/utils.py\u001b[0m in \u001b[0;36mplot_nns\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplot_nns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         ranked_sentances = sorted([self.num_nouns(self.document[i]) \\\n\u001b[0;32m--> 111\u001b[0;31m                         for i in range(len(self.document))], key=lambda x: x[1], reverse=True)\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranked_sentances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/eoshea/sflintro/text_sum/utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplot_nns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         ranked_sentances = sorted([self.num_nouns(self.document[i]) \\\n\u001b[0;32m--> 111\u001b[0;31m                         for i in range(len(self.document))], key=lambda x: x[1], reverse=True)\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranked_sentances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "summarizer_nmf.plot_nns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rank sentance importance by total document similarity\n",
    "summarizer_nmf.method = 'similarity'\n",
    "for line in summarizer_nmf.extract_summary(sentances):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_nmf.plot_dots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the summarization with LDA topic model, trained on only the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rank sentance importance by number of nouns\n",
    "summarizer_lda = utils.TextSummary(lda, num_out, vectorizer=tf_vectorizer, method='nns')\n",
    "for line in summarizer_lda.extract_summary(sentances):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_lda.plot_nns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rank sentance importance by total document similarity\n",
    "summarizer_lda.method = 'similarity'\n",
    "for line in summarizer_lda.extract_summary(sentances):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_lda.plot_dots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try the pre-trained doc2vec for topic model sentances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the pre-trained doc2vec\n",
    "doc2vec = utils.load_doc2vec()\n",
    "\n",
    "#Rank sentance importance by number of nouns\n",
    "summarizer_d2v = utils.TextSummary(doc2vec, num_out, vectorizer=None, method='nns')\n",
    "for line in summarizer_d2v.extract_summary(sentances):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_d2v.plot_nns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rank sentance importance by total document similarity\n",
    "summarizer_d2v.method = 'similarity'\n",
    "for line in summarizer_d2v.extract_summary(sentances):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_d2v.plot_dots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
